// app/api/chat/route.ts - VERSI√ìN CORREGIDA GPT-5
import { NextRequest } from "next/server"
import { OpenAI } from "openai" // ‚úÖ Importaci√≥n correcta

export const maxDuration = 30

// ‚úÖ CONFIGURACI√ìN CORREGIDA
const MODELS = {
  primary: 'gpt-5-nano',
  fallback: 'gpt-4o', 
  useStreaming: true
}

// ‚úÖ INICIALIZAR CLIENTE OPENAI CORRECTAMENTE
const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
})

export async function POST(req: NextRequest) {
  console.log("=== INICIANDO CHAT API GPT-5 ===")
  
  try {
    // VERIFICAR API KEY
    if (!process.env.OPENAI_API_KEY) {
      console.error("‚ùå Falta OPENAI_API_KEY")
      return new Response("OpenAI API key not configured", { status: 500 })
    }

    let messages: any[] = []
    const contentType = req.headers.get("Content-Type") || ""

    // ... [Tu c√≥digo de parseo se mantiene igual] ...
    // PARSEAR REQUEST (JSON o FormData)
    if (contentType.includes("application/json")) {
      const { messages: reloadedMessages } = await req.json()
      console.log("üì® Request tipo JSON con", reloadedMessages?.length || 0, "mensajes")
      messages = reloadedMessages
    } else if (contentType.includes("multipart/form-data")) {
      // ... [c√≥digo formData igual] ...
    }

    console.log("üí¨ Total mensajes a enviar:", messages.length)

    // SYSTEM PROMPT [mantener igual]
    const systemPrompt = `Eres un asistente de IA emp√°tico y experto en acoso escolar...`

    // ‚úÖ FUNCI√ìN CORREGIDA PARA GPT-5
    async function tryModelGPT5(modelName: string, userType: string = 'parent') {
      console.log(`ü§ñ Intentando GPT-5: ${modelName}`)
      
      // ‚úÖ CONFIGURACI√ìN DIN√ÅMICA SEG√öN USUARIO
      const getOptimalConfig = (model: string, userType: string) => {
        const baseConfig = {
          model: model,
          messages: [
            { role: "system", content: systemPrompt },
            ...messages.map((msg: any) => ({
              role: msg.role,
              content: msg.content
            }))
          ],
          max_tokens: 1500,
          temperature: 0.7,
        }

        // ‚úÖ NUEVOS PAR√ÅMETROS GPT-5
        if (model.startsWith('gpt-5')) {
          return {
            ...baseConfig,
            // üî• VERBOSITY DIN√ÅMICO
            verbosity: userType === 'child' ? 'low' : 
                      userType === 'therapist' ? 'high' : 'medium',
            
            // ‚ö° REASONING EFFORT OPTIMIZADO  
            reasoning_effort: userType === 'emergency' ? 'minimal' : 'medium',
          }
        }
        
        return baseConfig
      }

      const config = getOptimalConfig(modelName, userType)
      
      // ‚úÖ USAR OPENAI SDK OFICIAL
      const response = await openai.chat.completions.create({
        ...config,
        stream: MODELS.useStreaming,
      })

      console.log(`‚úÖ ${modelName} funcionando correctamente`)
      return response
    }

    // ‚úÖ FUNCI√ìN FALLBACK PARA GPT-4
    async function tryModelGPT4(modelName: string) {
      console.log(`ü§ñ Fallback a: ${modelName}`)
      
      const response = await openai.chat.completions.create({
        model: modelName,
        messages: [
          { role: "system", content: systemPrompt },
          ...messages.map((msg: any) => ({
            role: msg.role,
            content: msg.content
          }))
        ],
        stream: MODELS.useStreaming,
        max_tokens: 1500,
        temperature: 0.7,
        presence_penalty: 0.6,
        frequency_penalty: 0.3
      })

      console.log(`‚úÖ ${modelName} funcionando correctamente`)
      return response
    }

    // ‚úÖ L√ìGICA DE RETRY MEJORADA
    let response
    let modelUsed = MODELS.primary
    
    try {
      // ‚úÖ INTENTAR GPT-5 PRIMERO
      response = await tryModelGPT5(MODELS.primary)
      modelUsed = MODELS.primary
    } catch (primaryError: any) {
      console.log("‚ö†Ô∏è GPT-5 fall√≥:", primaryError.message)
      
      try {
        // ‚úÖ FALLBACK A GPT-4
        response = await tryModelGPT4(MODELS.fallback)
        modelUsed = MODELS.fallback
      } catch (fallbackError: any) {
        console.error("‚ùå Ambos modelos fallaron")
        return new Response(JSON.stringify({
          error: "No se pudo conectar con ning√∫n modelo",
          details: {
            primary: primaryError.message,
            fallback: fallbackError.message
          }
        }), { status: 500 })
      }
    }

    console.log(`üéâ Usando modelo: ${modelUsed}`)

    // ‚úÖ MANEJAR RESPUESTA NO-STREAMING
    if (!MODELS.useStreaming) {
      const content = response.choices[0].message?.content || "Sin respuesta"
      return new Response(JSON.stringify({
        content,
        model: modelUsed
      }), {
        headers: { 'Content-Type': 'application/json' }
      })
    }

    // ‚úÖ MANEJAR STREAMING CORRECTAMENTE
    const encoder = new TextEncoder()

    const stream = new ReadableStream({
      async start(controller) {
        try {
          for await (const chunk of response) {
            const content = chunk.choices[0]?.delta?.content || ''
            
            if (content) {
              const formattedChunk = `0:${JSON.stringify({ content })}\n`
              controller.enqueue(encoder.encode(formattedChunk))
            }
          }
          
          // Enviar se√±al de finalizaci√≥n
          controller.enqueue(encoder.encode(`0:${JSON.stringify({ done: true, model: modelUsed })}\n`))
          controller.close()
          
        } catch (streamError) {
          console.error("‚ùå Error en streaming:", streamError)
          controller.close()
        }
      },
    })

    return new Response(stream, {
      headers: {
        'Content-Type': 'text/plain; charset=utf-8',
        'Cache-Control': 'no-cache',
        'Connection': 'keep-alive',
        'X-Model-Used': modelUsed
      },
    })

  } catch (error) {
    console.error("‚ùå Error general en chat API:", error)
    const errorMessage = error instanceof Error ? error.message : "Error desconocido"
    
    return new Response(JSON.stringify({ 
      error: "Error en chat API", 
      details: errorMessage,
      timestamp: new Date().toISOString()
    }), { 
      status: 500,
      headers: { 'Content-Type': 'application/json' }
    })
  }
}
